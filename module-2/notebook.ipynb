{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6d032b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\PRACTICALS\\4_LLM\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m2025-06-22 13:53:37.283\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mfastembed.embedding\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[33m\u001b[1mDefaultEmbedding, FlagEmbedding, JinaEmbedding are deprecated.Use from fastembed import TextEmbedding instead.\u001b[0m\n",
      "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "c:\\Users\\lenovo\\PRACTICALS\\4_LLM\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lenovo\\AppData\\Local\\Temp\\fastembed_cache\\models--xenova--jina-embeddings-v2-small-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "\u001b[32m2025-06-22 13:53:57.844\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mfastembed.common.model_management\u001b[0m:\u001b[36mdownload_model\u001b[0m:\u001b[36m430\u001b[0m - \u001b[31m\u001b[1mCould not download model from HuggingFace: [WinError 1314] A required privilege is not held by the client: '..\\\\..\\\\blobs\\\\d22c8575768152270e2472533dc444174773350b' -> 'C:\\\\Users\\\\lenovo\\\\AppData\\\\Local\\\\Temp\\\\fastembed_cache\\\\models--xenova--jina-embeddings-v2-small-en\\\\snapshots\\\\523cadcb9c2e71c7153fc46016e1fe79acb4f58f\\\\config.json' Falling back to other sources.\u001b[0m\n",
      "\u001b[32m2025-06-22 13:53:57.844\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mfastembed.common.model_management\u001b[0m:\u001b[36mdownload_model\u001b[0m:\u001b[36m452\u001b[0m - \u001b[31m\u001b[1mCould not download model from either source, sleeping for 3.0 seconds, 2 retries left.\u001b[0m\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00,  8.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (512,)\n",
      "Min value: -0.11726373885183883\n"
     ]
    }
   ],
   "source": [
    "from fastembed.embedding import TextEmbedding\n",
    "import numpy as np\n",
    "\n",
    "model = TextEmbedding(model_name=\"jinaai/jina-embeddings-v2-small-en\")\n",
    "query = [\"I just discovered the course. Can I join now?\"]\n",
    "embedding = next(model.embed(query))\n",
    "\n",
    "print(\"Shape:\", embedding.shape)\n",
    "print(\"Min value:\", np.min(embedding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42ee6dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "q = embedding  # this is a 512-dim vector\n",
    "similarity = q.dot(q)\n",
    "\n",
    "print(similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9075eee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.9008528895674548\n"
     ]
    }
   ],
   "source": [
    "from fastembed import TextEmbedding\n",
    "import numpy as np\n",
    "\n",
    "model = TextEmbedding(model_name=\"jinaai/jina-embeddings-v2-small-en\")\n",
    "\n",
    "# Get both embeddings\n",
    "q_vector = list(model.embed([\"I just discovered the course. Can I join now?\"]))[0]\n",
    "doc_vector = list(model.embed([\"Can I still join the course after the start date?\"]))[0]\n",
    "\n",
    "similarity = np.dot(q_vector, doc_vector)\n",
    "print(\"Cosine Similarity:\", similarity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8d7a376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 0 similarity: 0.7629684696540238\n",
      "Doc 1 similarity: 0.8182378150042889\n",
      "Doc 2 similarity: 0.7986967622088167\n",
      "Doc 3 similarity: 0.6783809181253402\n",
      "Doc 4 similarity: 0.73044992343336\n",
      "Most similar document index: 1\n"
     ]
    }
   ],
   "source": [
    "from fastembed import TextEmbedding\n",
    "import numpy as np\n",
    "\n",
    "query = 'I just discovered the course. Can I join now?'\n",
    "\n",
    "documents = [\n",
    "    {'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\"},\n",
    "    {'text': 'Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.'},\n",
    "    {'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00...\"},\n",
    "    {'text': 'You can start by installing and setting up all the dependencies and requirements:\\nGoogle cloud account\\nGoogle Cloud SDK...'},\n",
    "    {'text': 'Star the repo! Share it with friends if you find it useful ❣️\\nCreate a PR if you see you can improve the text or the structure of the repository.'}\n",
    "]\n",
    "\n",
    "# Initialize model\n",
    "model = TextEmbedding(model_name=\"jinaai/jina-embeddings-v2-small-en\")\n",
    "\n",
    "# Embed query and docs\n",
    "q = np.array(list(model.embed([query]))[0])\n",
    "V = np.array(list(model.embed([doc[\"text\"] for doc in documents])))\n",
    "\n",
    "# Compute cosine similarities\n",
    "cos_similarities = V.dot(q)\n",
    "\n",
    "# Print similarities and most relevant doc\n",
    "for i, score in enumerate(cos_similarities):\n",
    "    print(f\"Doc {i} similarity: {score}\")\n",
    "\n",
    "print(f\"Most similar document index: {np.argmax(cos_similarities)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "390670c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 0 similarity: 0.8514543236908068\n",
      "Doc 1 similarity: 0.843659415911307\n",
      "Doc 2 similarity: 0.8408287048502558\n",
      "Doc 3 similarity: 0.7755157969663907\n",
      "Doc 4 similarity: 0.8086007795043937\n",
      "Most similar document index (Q4): 0\n"
     ]
    }
   ],
   "source": [
    "from fastembed.embedding import TextEmbedding\n",
    "import numpy as np\n",
    "\n",
    "# Load the embedding model\n",
    "model = TextEmbedding(model_name=\"jinaai/jina-embeddings-v2-small-en\")\n",
    "\n",
    "# Define the query\n",
    "query = \"I just discovered the course. Can I join now?\"\n",
    "\n",
    "# Get the embedding for the query\n",
    "query_embedding = list(model.embed([query]))[0]\n",
    "\n",
    "# Define the documents\n",
    "documents = [\n",
    "    {'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\",\n",
    "     'section': 'General course-related questions',\n",
    "     'question': 'Course - Can I still join the course after the start date?',\n",
    "     'course': 'data-engineering-zoomcamp'},\n",
    "    {'text': 'Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.',\n",
    "     'section': 'General course-related questions',\n",
    "     'question': 'Course - Can I follow the course after it finishes?',\n",
    "     'course': 'data-engineering-zoomcamp'},\n",
    "    {'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
    "     'section': 'General course-related questions',\n",
    "     'question': 'Course - When will the course start?',\n",
    "     'course': 'data-engineering-zoomcamp'},\n",
    "    {'text': 'You can start by installing and setting up all the dependencies and requirements:\\nGoogle cloud account\\nGoogle Cloud SDK\\nPython 3 (installed with Anaconda)\\nTerraform\\nGit\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.',\n",
    "     'section': 'General course-related questions',\n",
    "     'question': 'Course - What can I do before the course starts?',\n",
    "     'course': 'data-engineering-zoomcamp'},\n",
    "    {'text': 'Star the repo! Share it with friends if you find it useful ❣️\\nCreate a PR if you see you can improve the text or the structure of the repository.',\n",
    "     'section': 'General course-related questions',\n",
    "     'question': 'How can we contribute to the course?',\n",
    "     'course': 'data-engineering-zoomcamp'}\n",
    "]\n",
    "\n",
    "# Concatenate question and text into a single field: full_text\n",
    "full_texts = [doc[\"question\"] + \" \" + doc[\"text\"] for doc in documents]\n",
    "\n",
    "# Embed all full_texts\n",
    "full_embeddings = list(model.embed(full_texts))\n",
    "full_V = np.array(full_embeddings)\n",
    "\n",
    "# Compute cosine similarity using dot product (vectors are normalized)\n",
    "similarities = full_V.dot(query_embedding)\n",
    "\n",
    "# Print similarity scores\n",
    "for i, score in enumerate(similarities):\n",
    "    print(f\"Doc {i} similarity: {score}\")\n",
    "\n",
    "# Find the most similar document\n",
    "best_index = np.argmax(similarities)\n",
    "print(\"Most similar document index (Q4):\", best_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f1ef36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]c:\\Users\\lenovo\\PRACTICALS\\4_LLM\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lenovo\\AppData\\Local\\Temp\\fastembed_cache\\models--Qdrant--bge-small-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:20<00:00,  4.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (384,)\n"
     ]
    }
   ],
   "source": [
    "from fastembed.embedding import TextEmbedding\n",
    "import numpy as np\n",
    "\n",
    "# Load the BAAI small model\n",
    "model = TextEmbedding(model_name=\"BAAI/bge-small-en\")\n",
    "\n",
    "# Embed a sample text\n",
    "embedding = list(model.embed([\"Test sentence\"]))[0]\n",
    "\n",
    "# Check the shape\n",
    "print(\"Embedding shape:\", embedding.shape)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1729ee48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_14212\\2829715944.py:30: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  qdrant.recreate_collection(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top score: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_14212\\2829715944.py:45: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = qdrant.search(collection_name=collection_name, query_vector=query_vector, limit=1)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from fastembed.embedding import TextEmbedding\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "import hashlib\n",
    "\n",
    "# Load ML Zoomcamp documents\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "documents_raw = requests.get(docs_url).json()\n",
    "\n",
    "documents = []\n",
    "for course in documents_raw:\n",
    "    if course['course'] == 'machine-learning-zoomcamp':\n",
    "        for doc in course['documents']:\n",
    "            doc['course'] = course['course']\n",
    "            documents.append(doc)\n",
    "\n",
    "# Use the small model\n",
    "model = TextEmbedding(model_name=\"BAAI/bge-small-en\")\n",
    "texts = [doc['question'] + ' ' + doc['text'] for doc in documents]\n",
    "embeddings = list(model.embed(texts))\n",
    "\n",
    "# Setup Qdrant\n",
    "qdrant = QdrantClient(\"http://localhost:6333\")\n",
    "collection_name = \"ml-zoomcamp-faq\"\n",
    "\n",
    "if collection_name in [c.name for c in qdrant.get_collections().collections]:\n",
    "    qdrant.delete_collection(collection_name=collection_name)\n",
    "\n",
    "qdrant.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "# Add data to Qdrant\n",
    "points = []\n",
    "for doc, embedding in zip(documents, embeddings):\n",
    "    uid = int(hashlib.md5((doc['course'] + doc['question']).encode()).hexdigest(), 16) % (10**16)\n",
    "    points.append(PointStruct(id=uid, vector=embedding, payload=doc))\n",
    "qdrant.upsert(collection_name=collection_name, points=points)\n",
    "\n",
    "# Run search\n",
    "query = \"I just discovered the course. Can I join now?\"\n",
    "query_vector = list(model.embed([query]))[0]\n",
    "search_result = qdrant.search(collection_name=collection_name, query_vector=query_vector, limit=1)\n",
    "\n",
    "print(f\"Top score: {search_result[0].score:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
